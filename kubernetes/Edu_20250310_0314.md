# https://www.cccr-edu.or.kr/course/course_view.jsp?id=176365

# https://kubernetes.io/docs/home/

## 쿠버네티스 교육과정

---

#### 강사님 email

juhyokim@nobreak.kr

wifi cccr204-classroom2 : cccr5678

http://192.168.4.72

##### 구글드라이브

https://iii.ad/2c812d

---

### 1 일차

1. 이론교육 진행

export TEMPLATE_ID=9003
export VM_ID=131
export VM_NAME=sjh-rke-master1
export VM_NIC=vmbr0
export VM_IP=192.168.0.131/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 4096 --cores 4
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=132
export VM_NAME=sjh-rke-work1
export VM_NIC=vmbr0
export VM_IP=192.168.0.132/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 8192 --cores 8
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=133
export VM_NAME=sjh-rke-work2
export VM_NIC=vmbr0
export VM_IP=192.168.0.133/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 8192 --cores 8
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=130
export VM_NAME=sjh-rke-depoly
export VM_NIC=vmbr0
export VM_IP=192.168.0.130/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 4096 --cores 4
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

---

#### kubespray

https://kubespray.io/#/

ssh-keygen
ssh-copy-id vagrant@192.168.56.21
sudo apt update
sudo apt install -y git pip python3.10-venv
python3 -m venv venv
source venv/bin/activate
git clone --single-branch --branch release-2.27 https://github.com/kubernetes-sigs/kuberspray.git
cd kubespray
pip install -r requirements.txt
cp -rp inventory/sample inventory/mycluster
vim inventory/mycluster/inventory.ini
vim inventory/mycluster/group_vars/k8s_clyster/addons.yml
vim inventory/mycluster/group_vars/k8s_clyster/k8s-cluster.yml
ansible-playbook -i inventory/mycluster/inventory.ini -b cluster.yml
mkdir ~/.kube
sudo cp /etc/kubernetes/admin.conf ~/.kube/config
sudo chown $USER:$GROUP ~/.kube/config

```bash
helm repo add metallb https://metallb.github.io/frr-k8s
helm repo update metallb
helm search repo metallb
helm pull metallb/metallb
```

### 자동완성 기능

kubectl completion bash | sudo tee /etc/bash_completion.d/test

### yaml 생성

---

k api-resources
NAME SHORTNAMES APIVERSION NAMESPACED KIND
bindings v1 true Binding
componentstatuses cs v1 false ComponentStatus
configmaps cm v1 true ConfigMap
endpoints ep v1 true Endpoints
events ev v1 true Event
limitranges limits v1 true LimitRange
namespaces ns v1 false Namespace
nodes no v1 false Node
persistentvolumeclaims pvc v1 true PersistentVolumeClaim  
persistentvolumes pv v1 false PersistentVolume

### pods po v1 true Pod

# k explain po

# KIND:

# Pod

# VERSION: v1

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myweb-pod
  labls:
    app: myweb
    type: fronted
spec:
  containers:
    - name: nginx-container
      image: nginx
```

kubectl get po myweb-pod -o wide
kubectl get po myweb-pod -o yaml
kubectl logs myweb-pod
kubectl exec po myweb-pod

#### shell 접속

kubectl exec -it myweb-pod -- /bin/sh
kubectl exex -it myweb-pod -- ls

#### 컨테이너 2개 이상

Usage:
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...] [options]

#### Lables 확인

kubectl get po -L type,app
kubectl get po --show-labels
kubectl get svc --show-labels
kubectl get no --show-labels sjh-rke-depoly

kubectl label pod myweb-pod ver=1.0
pod/myweb-pod labeled
kubectl label pod myweb-pod ver=2.0
error: 'ver' already has a value (1.0), and --overwrite is false
kubectl label pod myweb-pod ver=2.0 --overwrite
pod/myweb-pod labeled

kubectl label pod myweb-pod ver=1.0
pod/myweb-pod labeled
kubectl label pod myweb-pod ver=2.0
error: 'ver' already has a value (1.0), and --overwrite is false
kubectl label pod myweb-pod ver=2.0 --overwrite
pod/myweb-pod labeled

ubuntu@sjh-rke-depoly:~$ kubectl get pod -l type=backed
NAME READY STATUS RESTARTS AGE
myweb-pod 1/1 Running 0 8m14s
ubuntu@sjh-rke-depoly:~$ kubectl get pod -l type
NAME READY STATUS RESTARTS AGE
myweb-pod 1/1 Running 0 8m30s

#### apply 명령

kubectl apply -f pod.yaml
pod/myweb-pod created
kubectl apply -f pod.yaml
pod/myweb-pod unchanged

kubectl get pod --show-labels
NAME READY STATUS RESTARTS AGE LABELS
myweb-pod 1/1 Running 0 84s app=myweb,type=fronted

kubectl apply -f pod.yaml
pod/myweb-pod configured

kubectl get pod --show-labels
NAME READY STATUS RESTARTS AGE LABELS
myweb-pod 1/1 Running 0 2m17s app=myweb,type=backed

#### name space

ubuntu@sjh-rke-depoly:~$ k get ns
NAME STATUS AGE
cattle-fleet-clusters-system Active 13m
cattle-fleet-local-system Active 13m
cattle-fleet-system Active 14m
cattle-global-data Active 14m
cattle-global-nt Active 14m
cattle-impersonation-system Active 14m
cattle-provisioning-capi-system Active 12m
cattle-system Active 16m
cattle-ui-plugin-system Active 14m
cert-manager Active 4h10m
cluster-fleet-local-local-1a3d67d0a899 Active 13m
default Active 4h40m
fleet-default Active 14m
fleet-local Active 14m
kube-node-lease Active 4h40m
kube-public Active 4h40m
kube-system Active 4h40m
local Active 14m
p-sfbbq Active 14m
p-svkff Active 14m
user-nhjdr Active 10m

ubuntu@sjh-rke-depoly:~$ k get all -n kube-system
NAME READY STATUS RESTARTS AGE
pod/cloud-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
pod/etcd-sjh-rke-depoly 1/1 Running 0 4h42m
pod/helm-install-rke2-canal-ghxf5 0/1 Completed 0 4h42m
pod/helm-install-rke2-coredns-5l6sf 0/1 Completed 0 4h42m
pod/helm-install-rke2-ingress-nginx-z8xb4 0/1 Completed 0 4h42m
pod/helm-install-rke2-metrics-server-hctw8 0/1 Completed 0 4h42m
pod/helm-install-rke2-runtimeclasses-75d2w 0/1 Completed 0 4h42m
pod/helm-install-rke2-snapshot-controller-656xm 0/1 Completed 2 4h42m
pod/helm-install-rke2-snapshot-controller-crd-tpfpd 0/1 Completed 0 4h42m
pod/kube-apiserver-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-proxy-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-proxy-sjh-rke-master1 1/1 Running 0 4h22m
pod/kube-proxy-sjh-rke-work1 1/1 Running 0 4h24m
pod/kube-proxy-sjh-rke-work2 1/1 Running 0 4h22m
pod/kube-scheduler-sjh-rke-depoly 1/1 Running 0 4h42m
pod/rke2-canal-8kw4q 2/2 Running 0 4h24m
pod/rke2-canal-9dj48 2/2 Running 0 4h42m
pod/rke2-canal-9fb5p 2/2 Running 0 4h24m
pod/rke2-canal-zqfrd 2/2 Running 0 4h24m
pod/rke2-coredns-rke2-coredns-55bdf87668-bk8dl 1/1 Running 0 4h42m
pod/rke2-coredns-rke2-coredns-55bdf87668-hbm6r 1/1 Running 0 4h24m
pod/rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64-pqxf5 1/1 Running 0 4h42m
pod/rke2-ingress-nginx-controller-88rf5 1/1 Running 0 4h41m
pod/rke2-ingress-nginx-controller-csvks 1/1 Running 0 4h22m
pod/rke2-ingress-nginx-controller-jbmkb 1/1 Running 0 4h22m
pod/rke2-ingress-nginx-controller-nm2dr 1/1 Running 0 4h22m
pod/rke2-metrics-server-58ff89f9c7-gjp7p 1/1 Running 0 4h41m
pod/rke2-snapshot-controller-58dbcfd956-gdh2h 1/1 Running 0 4h41m

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/rke2-coredns-rke2-coredns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP 4h42m
service/rke2-ingress-nginx-controller-admission ClusterIP 10.43.68.120 <none> 443/TCP 4h41m
service/rke2-metrics-server ClusterIP 10.43.150.234 <none> 443/TCP 4h41m

NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE
daemonset.apps/rke2-canal 4 4 4 4 4 kubernetes.io/os=linux 4h42m
daemonset.apps/rke2-ingress-nginx-controller 4 4 4 4 4 kubernetes.io/os=linux 4h41m

NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/rke2-coredns-rke2-coredns 2/2 2 2 4h42m
deployment.apps/rke2-coredns-rke2-coredns-autoscaler 1/1 1 1 4h42m
deployment.apps/rke2-metrics-server 1/1 1 1 4h41m
deployment.apps/rke2-snapshot-controller 1/1 1 1 4h41m

NAME DESIRED CURRENT READY AGE
replicaset.apps/rke2-coredns-rke2-coredns-55bdf87668 2 2 2 4h42m
replicaset.apps/rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64 1 1 1 4h42m
replicaset.apps/rke2-metrics-server-58ff89f9c7 1 1 1 4h41m
replicaset.apps/rke2-snapshot-controller-58dbcfd956 1 1 1 4h41m

NAME STATUS COMPLETIONS DURATION AGE
job.batch/helm-install-rke2-canal Complete 1/1 30s 4h42m
job.batch/helm-install-rke2-coredns Complete 1/1 30s 4h42m
job.batch/helm-install-rke2-ingress-nginx Complete 1/1 89s 4h42m
job.batch/helm-install-rke2-metrics-server Complete 1/1 74s 4h42m
job.batch/helm-install-rke2-runtimeclasses Complete 1/1 75s 4h42m
job.batch/helm-install-rke2-snapshot-controller Complete 1/1 88s 4h42m
job.batch/helm-install-rke2-snapshot-controller-crd Complete 1/1 75s 4h42m

##### kube-system pod

ubuntu@sjh-rke-depoly:~$ k get po -n kube-system
NAME READY STATUS RESTARTS AGE
cloud-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
etcd-sjh-rke-depoly 1/1 Running 0 4h42m
helm-install-rke2-canal-ghxf5 0/1 Completed 0 4h43m
helm-install-rke2-coredns-5l6sf 0/1 Completed 0 4h43m
helm-install-rke2-ingress-nginx-z8xb4 0/1 Completed 0 4h43m
helm-install-rke2-metrics-server-hctw8 0/1 Completed 0 4h43m
helm-install-rke2-runtimeclasses-75d2w 0/1 Completed 0 4h43m
helm-install-rke2-snapshot-controller-656xm 0/1 Completed 2 4h43m
helm-install-rke2-snapshot-controller-crd-tpfpd 0/1 Completed 0 4h43m
kube-apiserver-sjh-rke-depoly 1/1 Running 0 4h42m
kube-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
kube-proxy-sjh-rke-depoly 1/1 Running 0 4h42m
kube-proxy-sjh-rke-master1 1/1 Running 0 4h23m
kube-proxy-sjh-rke-work1 1/1 Running 0 4h25m
kube-proxy-sjh-rke-work2 1/1 Running 0 4h23m
kube-scheduler-sjh-rke-depoly 1/1 Running 0 4h42m
rke2-canal-8kw4q 2/2 Running 0 4h25m
rke2-canal-9dj48 2/2 Running 0 4h42m
rke2-canal-9fb5p 2/2 Running 0 4h25m
rke2-canal-zqfrd 2/2 Running 0 4h25m
rke2-coredns-rke2-coredns-55bdf87668-bk8dl 1/1 Running 0 4h42m
rke2-coredns-rke2-coredns-55bdf87668-hbm6r 1/1 Running 0 4h25m
rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64-pqxf5 1/1 Running 0 4h42m
rke2-ingress-nginx-controller-88rf5 1/1 Running 0 4h41m
rke2-ingress-nginx-controller-csvks 1/1 Running 0 4h23m
rke2-ingress-nginx-controller-jbmkb 1/1 Running 0 4h22m
rke2-ingress-nginx-controller-nm2dr 1/1 Running 0 4h22m
rke2-metrics-server-58ff89f9c7-gjp7p 1/1 Running 0 4h42m
rke2-snapshot-controller-58dbcfd956-gdh2h 1/1 Running 0 4h41m

ubuntu@sjh-rke-depoly:~$ k get ns matallb
NAME STATUS AGE
matallb Active 15s

### 프로브

---

apiVersion: v1
kind: Pod
metadata:
name: myweb-pod
annotations:
name: test annotation
labels:
app: myweb
type: backed
spec:
containers: - name: nginx-container
image: ghcr.io/c1t1d0s7/go-myweb:alpine
livenessProve:
httpGet:
path: /health
port: 8080  
...

sample

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
    - name: liveness
      image: registry.k8s.io/busybox:1.27.2
      args:
        - /bin/sh
        - -c
        - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
      livenessProbe:
        exec:
          command:
            - cat
            - /tmp/healthy
        initialDelaySeconds: 5
        periodSeconds: 5
```

# 2일차

https://v1-31.docs.kubernetes.io/ko/docs/concepts/workloads/

1. 워크로드 리소스 종류
2. 리플리케이션 컨트롤러
3. 리플리카셋 파드의 복제본 개수 관리
4. 디플로이 먼트 리플리카 셋 버전 배포 관리
5. 스테이플셋 파드의 상태(순서,정보)를 유지할 수 있는 관리 방식
6. Daemonset - 파드를 각 노드 당 하나씩만 배포
7. JOB 일회성 애플리케이션을 지정한 완료 횟수만큼 보장
8. cronjob 일회성 애플리케이션 반복 실행행

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # 케이스에 따라 레플리카를 수정한다.
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
        - name: php-redis
          image: gcr.io/google_samples/gb-frontend:v3
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app1
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app1
  template:
    metadata:
      labels:
        app: nginx-app1
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: app1-vol
              mountPath: /usr/share/nginx/html
      volumes:
        - name: app1-vol
          configMap:
            name: app1-html

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc1
  namespace: test
spec:
  selector:
    app: nginx-app1
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app2
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app2
  template:
    metadata:
      labels:
        app: nginx-app2
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: app2-vol
              mountPath: /usr/share/nginx/html
      volumes:
        - name: app2-vol
          configMap:
            name: app2-html

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc2
  namespace: test
spec:
  selector:
    app: nginx-app2
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP
```

```yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
        - name: pi
          image: perl:5.34.0
          command: ['perl', '-Mbignum=bpi', '-wle', 'print bpi(2000)']
      restartPolicy: OnFailure
  backoffLimit: 4
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myapp-job-para
spec:
  completions: 10
  parallelism: 3
  template:
    metadata:
      labels:
        app: myapp-job-para
    spec:
      restartPolicy: OnFailure
      containers:
        - name: sleep
          image: registry.k8s.io/busybox:1.27.2
          args:
            - /bin/sh
            - c
            - sleep 10
```

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: myapp-cj
spec:
  #  schedule: 분 시 일 월 요일
  #  completions: 10
  #  parallelism: 3
  schedule: '* * * * *'
  jobTemplate:
    spec:
      #    activeDeadlineSeconds: 10
      #   backoffLimit: 3
      template:
        metadata:
          labels:
            app: myapp-cj
        spec:
          restartPolicy: OnFailure
          containers:
            - name: sleep
              image: registry.k8s.io/busybox:1.27.2
              args:
                - /bin/sh
                - c
                - sleep 10
```

1. CLusterIP 내부용 서비스(파드 간의 통신)
   - HeadLessServic 서비스 자체에 IP 주소가 할당 X
2. NodePort 외부용 서비스(각 노드 IP 주소로 접근 (특정 포트))
3. LoadBalancer 외부용 서비스(별도의 LB 장치를 통해 접속)
   - Metal-LB 로 구현 / 클라우드는 별도의 LB 서비스 사용 / 물리 LB를 이용해 사용
4. ExternalName 1,2,3 서비스는 POD 접근 이지만 이 서비스는 파드에서 외부로 통신 시 사용 (CNAME)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
spec:
  selector:
    app: myapp-rs
  ports:
    - name: http
      port: 80 # Service Port
      targetPort: 8080 # pod Port
    - name: https
      port: 80 # Service Port
      targetPort: 8080 # pod Port
```

```yaml
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: myapp-rs-port
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - myapp-rs-port
          - backend
  #    matchLabels:
  #      tier: frontend
  template:
    metadata:
      labels:
        app: myapp-rs-port
    spec:
      containers:
        - name: myapp
          image: ghcr.io/c1t1d0s7/go-myweb:alpine
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  name: svc-port
spec:
  ports:
    - port: 80
      targetPort: http
  selector:
    app: myapp-rs-port
```

kubectl run nettool -it --image=ghcr.io/c1t1d0s7/network-multitool --rm bash

```yaml
---
apiVersion: networking.k8s.io/va
kind: Ingress
metadata:
  name: myapp-ing
spec:
  defaultBackend:
    service:
      name: myapp-svc-np
      port:
        number: 80
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-svc-np
                port:
                  number: 80
```

---

# 3일차

Review

1. 쿠버네티스 개요
2. MSA : 모듈화 구성
3. DevOps : 개발 + 운영
4. Containers 가상화 : 프로세스 단위의 격리
   - docker, podman, containerd, cri-o (컨테이너 런타임 : 컨테이너 실행/삭제)
5. 컨테이너 오케스트레이션(관리플랫폼 : 쿠버네티스, docker swam)
6. 쿠버네티스 아키텍쳐 : controlplane, worker(node)

   - controlplane : 2 CPU, 2048(4096~) : 쿠버네티스 클러스터를 운영/관리 : 최소 1대 3대 이상(홀수:투표 시스템 때문) 권장
     - 구성요소 :
       - kube-apiserver : 명령어를 포함해 각 개체들이 통신 시 사용
       - kube-scheduler : 스케쥴링 기법 혹은 파드에 대한 리소스 할당이 있을 때 컨테이너(파드)를 실행 할 수 있는 노드를 선택해서 배치 : 정책 설정 가능
       - kube-controller-manager : 구성요소들의 관리
       - etcd :클러스터 구성 정보 등을 저장하는 저장소 (3대 이상 권장), 백업 복구도 지원은 함
       - cloud-controller-manager : 필수는 아님, 클라우드 구성요소와 연계 할 때 사용
   - node : 2CPU, 2048(운영에 필요한 만큼) : 컨테이너 실행 및 운영(리소스 제공) : 최소 2대 이상 권장(가용성을 위해) - kubelet : 노드, 컨테이터(파드) 상태 확인 하는 역할 수행 - kube-proxy : 각각의 파드에 대한 네트워크 구성 책임 - container-runtime : 컨테이너 생성, 삭제 수행

7. Workload resource
   - Pods : 애플리케이션 배포 중 최소 단위 : 1개 이상의 컨테이너로 구성 : 보편적 테스트 용으로 배포(워크로드 리소스로 배포)
   - Workload resource(controller) : replication-controller(지금은 잘 안쓰임 : deployment, replicaset으로 변환)
     - replicaset(rs) : 파드의 복제본 갯수를 설정 및 유지 (절대적 갯수)
     - Deamonset(ds) : 각 노드마다 Pod를 하나씩 생성 및 유지 (절대적 갯수)
     - Job : 일회성 작업 -> 완료 횟수를 보장
     - CronJob : 일회성 작업의 주기적으로 실행행
8.
9.

---
