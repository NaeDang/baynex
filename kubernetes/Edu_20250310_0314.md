# https://www.cccr-edu.or.kr/course/course_view.jsp?id=176365

# https://kubernetes.io/docs/home/

## 쿠버네티스 교육과정

---

#### 강사님 email

juhyokim@nobreak.kr

wifi cccr204-classroom2 : cccr5678

http://192.168.4.72

##### 구글드라이브

https://iii.ad/2c812d

---

### 1 일차

1. 이론교육 진행

export TEMPLATE_ID=9003
export VM_ID=131
export VM_NAME=sjh-rke-master1
export VM_NIC=vmbr0
export VM_IP=192.168.0.131/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 4096 --cores 4
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=132
export VM_NAME=sjh-rke-work1
export VM_NIC=vmbr0
export VM_IP=192.168.0.132/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 8192 --cores 8
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=133
export VM_NAME=sjh-rke-work2
export VM_NIC=vmbr0
export VM_IP=192.168.0.133/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 8192 --cores 8
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

export TEMPLATE_ID=9003
export VM_ID=130
export VM_NAME=sjh-rke-depoly
export VM_NIC=vmbr0
export VM_IP=192.168.0.130/24
export VM_GW=192.168.0.1
export STORAGE=local-lvm
qm clone $TEMPLATE_ID $VM_ID --name $VM_NAME --full true
qm set $VM_ID --machine q35,viommu=intel
qm set $VM_ID --bios ovmf --efidisk0 $STORAGE:1,efitype=4m,pre-enrolled-keys=0
qm set $VM_ID --memory 4096 --cores 4
qm set $VM_ID --net0 virtio,bridge=$VM_NIC,firewall=1
qm set $VM_ID --ipconfig0 ip=$VM_IP,gw=$VM_GW
qm set $VM_ID --ciupgrade 0
qm resize $VM_ID scsi0 +100G
qm snapshot $VM_ID Pre

---

#### kubespray

https://kubespray.io/#/

ssh-keygen
ssh-copy-id vagrant@192.168.56.21
sudo apt update
sudo apt install -y git pip python3.10-venv
python3 -m venv venv
source venv/bin/activate
git clone --single-branch --branch release-2.27 https://github.com/kubernetes-sigs/kuberspray.git
cd kubespray
pip install -r requirements.txt
cp -rp inventory/sample inventory/mycluster
vim inventory/mycluster/inventory.ini
vim inventory/mycluster/group_vars/k8s_clyster/addons.yml
vim inventory/mycluster/group_vars/k8s_clyster/k8s-cluster.yml
ansible-playbook -i inventory/mycluster/inventory.ini -b cluster.yml
mkdir ~/.kube
sudo cp /etc/kubernetes/admin.conf ~/.kube/config
sudo chown $USER:$GROUP ~/.kube/config

```bash
helm repo add metallb https://metallb.github.io/frr-k8s
helm repo update metallb
helm search repo metallb
helm pull metallb/metallb
```

### 자동완성 기능

kubectl completion bash | sudo tee /etc/bash_completion.d/test

### yaml 생성

---

k api-resources
NAME SHORTNAMES APIVERSION NAMESPACED KIND
bindings v1 true Binding
componentstatuses cs v1 false ComponentStatus
configmaps cm v1 true ConfigMap
endpoints ep v1 true Endpoints
events ev v1 true Event
limitranges limits v1 true LimitRange
namespaces ns v1 false Namespace
nodes no v1 false Node
persistentvolumeclaims pvc v1 true PersistentVolumeClaim  
persistentvolumes pv v1 false PersistentVolume

### pods po v1 true Pod

# k explain po

# KIND:

# Pod

# VERSION: v1

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myweb-pod
  labls:
    app: myweb
    type: fronted
spec:
  containers:
    - name: nginx-container
      image: nginx
```

kubectl get po myweb-pod -o wide
kubectl get po myweb-pod -o yaml
kubectl logs myweb-pod
kubectl exec po myweb-pod

#### shell 접속

kubectl exec -it myweb-pod -- /bin/sh
kubectl exex -it myweb-pod -- ls

#### 컨테이너 2개 이상

Usage:
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...] [options]

#### Lables 확인

kubectl get po -L type,app
kubectl get po --show-labels
kubectl get svc --show-labels
kubectl get no --show-labels sjh-rke-depoly

kubectl label pod myweb-pod ver=1.0
pod/myweb-pod labeled
kubectl label pod myweb-pod ver=2.0
error: 'ver' already has a value (1.0), and --overwrite is false
kubectl label pod myweb-pod ver=2.0 --overwrite
pod/myweb-pod labeled

kubectl label pod myweb-pod ver=1.0
pod/myweb-pod labeled
kubectl label pod myweb-pod ver=2.0
error: 'ver' already has a value (1.0), and --overwrite is false
kubectl label pod myweb-pod ver=2.0 --overwrite
pod/myweb-pod labeled

ubuntu@sjh-rke-depoly:~$ kubectl get pod -l type=backed
NAME READY STATUS RESTARTS AGE
myweb-pod 1/1 Running 0 8m14s
ubuntu@sjh-rke-depoly:~$ kubectl get pod -l type
NAME READY STATUS RESTARTS AGE
myweb-pod 1/1 Running 0 8m30s

#### apply 명령

kubectl apply -f pod.yaml
pod/myweb-pod created
kubectl apply -f pod.yaml
pod/myweb-pod unchanged

kubectl get pod --show-labels
NAME READY STATUS RESTARTS AGE LABELS
myweb-pod 1/1 Running 0 84s app=myweb,type=fronted

kubectl apply -f pod.yaml
pod/myweb-pod configured

kubectl get pod --show-labels
NAME READY STATUS RESTARTS AGE LABELS
myweb-pod 1/1 Running 0 2m17s app=myweb,type=backed

#### name space

ubuntu@sjh-rke-depoly:~$ k get ns
NAME STATUS AGE
cattle-fleet-clusters-system Active 13m
cattle-fleet-local-system Active 13m
cattle-fleet-system Active 14m
cattle-global-data Active 14m
cattle-global-nt Active 14m
cattle-impersonation-system Active 14m
cattle-provisioning-capi-system Active 12m
cattle-system Active 16m
cattle-ui-plugin-system Active 14m
cert-manager Active 4h10m
cluster-fleet-local-local-1a3d67d0a899 Active 13m
default Active 4h40m
fleet-default Active 14m
fleet-local Active 14m
kube-node-lease Active 4h40m
kube-public Active 4h40m
kube-system Active 4h40m
local Active 14m
p-sfbbq Active 14m
p-svkff Active 14m
user-nhjdr Active 10m

ubuntu@sjh-rke-depoly:~$ k get all -n kube-system
NAME READY STATUS RESTARTS AGE
pod/cloud-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
pod/etcd-sjh-rke-depoly 1/1 Running 0 4h42m
pod/helm-install-rke2-canal-ghxf5 0/1 Completed 0 4h42m
pod/helm-install-rke2-coredns-5l6sf 0/1 Completed 0 4h42m
pod/helm-install-rke2-ingress-nginx-z8xb4 0/1 Completed 0 4h42m
pod/helm-install-rke2-metrics-server-hctw8 0/1 Completed 0 4h42m
pod/helm-install-rke2-runtimeclasses-75d2w 0/1 Completed 0 4h42m
pod/helm-install-rke2-snapshot-controller-656xm 0/1 Completed 2 4h42m
pod/helm-install-rke2-snapshot-controller-crd-tpfpd 0/1 Completed 0 4h42m
pod/kube-apiserver-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-proxy-sjh-rke-depoly 1/1 Running 0 4h42m
pod/kube-proxy-sjh-rke-master1 1/1 Running 0 4h22m
pod/kube-proxy-sjh-rke-work1 1/1 Running 0 4h24m
pod/kube-proxy-sjh-rke-work2 1/1 Running 0 4h22m
pod/kube-scheduler-sjh-rke-depoly 1/1 Running 0 4h42m
pod/rke2-canal-8kw4q 2/2 Running 0 4h24m
pod/rke2-canal-9dj48 2/2 Running 0 4h42m
pod/rke2-canal-9fb5p 2/2 Running 0 4h24m
pod/rke2-canal-zqfrd 2/2 Running 0 4h24m
pod/rke2-coredns-rke2-coredns-55bdf87668-bk8dl 1/1 Running 0 4h42m
pod/rke2-coredns-rke2-coredns-55bdf87668-hbm6r 1/1 Running 0 4h24m
pod/rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64-pqxf5 1/1 Running 0 4h42m
pod/rke2-ingress-nginx-controller-88rf5 1/1 Running 0 4h41m
pod/rke2-ingress-nginx-controller-csvks 1/1 Running 0 4h22m
pod/rke2-ingress-nginx-controller-jbmkb 1/1 Running 0 4h22m
pod/rke2-ingress-nginx-controller-nm2dr 1/1 Running 0 4h22m
pod/rke2-metrics-server-58ff89f9c7-gjp7p 1/1 Running 0 4h41m
pod/rke2-snapshot-controller-58dbcfd956-gdh2h 1/1 Running 0 4h41m

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/rke2-coredns-rke2-coredns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP 4h42m
service/rke2-ingress-nginx-controller-admission ClusterIP 10.43.68.120 <none> 443/TCP 4h41m
service/rke2-metrics-server ClusterIP 10.43.150.234 <none> 443/TCP 4h41m

NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE
daemonset.apps/rke2-canal 4 4 4 4 4 kubernetes.io/os=linux 4h42m
daemonset.apps/rke2-ingress-nginx-controller 4 4 4 4 4 kubernetes.io/os=linux 4h41m

NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/rke2-coredns-rke2-coredns 2/2 2 2 4h42m
deployment.apps/rke2-coredns-rke2-coredns-autoscaler 1/1 1 1 4h42m
deployment.apps/rke2-metrics-server 1/1 1 1 4h41m
deployment.apps/rke2-snapshot-controller 1/1 1 1 4h41m

NAME DESIRED CURRENT READY AGE
replicaset.apps/rke2-coredns-rke2-coredns-55bdf87668 2 2 2 4h42m
replicaset.apps/rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64 1 1 1 4h42m
replicaset.apps/rke2-metrics-server-58ff89f9c7 1 1 1 4h41m
replicaset.apps/rke2-snapshot-controller-58dbcfd956 1 1 1 4h41m

NAME STATUS COMPLETIONS DURATION AGE
job.batch/helm-install-rke2-canal Complete 1/1 30s 4h42m
job.batch/helm-install-rke2-coredns Complete 1/1 30s 4h42m
job.batch/helm-install-rke2-ingress-nginx Complete 1/1 89s 4h42m
job.batch/helm-install-rke2-metrics-server Complete 1/1 74s 4h42m
job.batch/helm-install-rke2-runtimeclasses Complete 1/1 75s 4h42m
job.batch/helm-install-rke2-snapshot-controller Complete 1/1 88s 4h42m
job.batch/helm-install-rke2-snapshot-controller-crd Complete 1/1 75s 4h42m

##### kube-system pod

ubuntu@sjh-rke-depoly:~$ k get po -n kube-system
NAME READY STATUS RESTARTS AGE
cloud-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
etcd-sjh-rke-depoly 1/1 Running 0 4h42m
helm-install-rke2-canal-ghxf5 0/1 Completed 0 4h43m
helm-install-rke2-coredns-5l6sf 0/1 Completed 0 4h43m
helm-install-rke2-ingress-nginx-z8xb4 0/1 Completed 0 4h43m
helm-install-rke2-metrics-server-hctw8 0/1 Completed 0 4h43m
helm-install-rke2-runtimeclasses-75d2w 0/1 Completed 0 4h43m
helm-install-rke2-snapshot-controller-656xm 0/1 Completed 2 4h43m
helm-install-rke2-snapshot-controller-crd-tpfpd 0/1 Completed 0 4h43m
kube-apiserver-sjh-rke-depoly 1/1 Running 0 4h42m
kube-controller-manager-sjh-rke-depoly 1/1 Running 0 4h42m
kube-proxy-sjh-rke-depoly 1/1 Running 0 4h42m
kube-proxy-sjh-rke-master1 1/1 Running 0 4h23m
kube-proxy-sjh-rke-work1 1/1 Running 0 4h25m
kube-proxy-sjh-rke-work2 1/1 Running 0 4h23m
kube-scheduler-sjh-rke-depoly 1/1 Running 0 4h42m
rke2-canal-8kw4q 2/2 Running 0 4h25m
rke2-canal-9dj48 2/2 Running 0 4h42m
rke2-canal-9fb5p 2/2 Running 0 4h25m
rke2-canal-zqfrd 2/2 Running 0 4h25m
rke2-coredns-rke2-coredns-55bdf87668-bk8dl 1/1 Running 0 4h42m
rke2-coredns-rke2-coredns-55bdf87668-hbm6r 1/1 Running 0 4h25m
rke2-coredns-rke2-coredns-autoscaler-65c8c6bd64-pqxf5 1/1 Running 0 4h42m
rke2-ingress-nginx-controller-88rf5 1/1 Running 0 4h41m
rke2-ingress-nginx-controller-csvks 1/1 Running 0 4h23m
rke2-ingress-nginx-controller-jbmkb 1/1 Running 0 4h22m
rke2-ingress-nginx-controller-nm2dr 1/1 Running 0 4h22m
rke2-metrics-server-58ff89f9c7-gjp7p 1/1 Running 0 4h42m
rke2-snapshot-controller-58dbcfd956-gdh2h 1/1 Running 0 4h41m

ubuntu@sjh-rke-depoly:~$ k get ns matallb
NAME STATUS AGE
matallb Active 15s

### 프로브

---

apiVersion: v1
kind: Pod
metadata:
name: myweb-pod
annotations:
name: test annotation
labels:
app: myweb
type: backed
spec:
containers: - name: nginx-container
image: ghcr.io/c1t1d0s7/go-myweb:alpine
livenessProve:
httpGet:
path: /health
port: 8080  
...

sample

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
    - name: liveness
      image: registry.k8s.io/busybox:1.27.2
      args:
        - /bin/sh
        - -c
        - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
      livenessProbe:
        exec:
          command:
            - cat
            - /tmp/healthy
        initialDelaySeconds: 5
        periodSeconds: 5
```

# 2일차

https://v1-31.docs.kubernetes.io/ko/docs/concepts/workloads/

1. 워크로드 리소스 종류
2. 리플리케이션 컨트롤러
3. 리플리카셋 pod의 복제본 개수 관리
4. 디플로이 먼트 리플리카 셋 버전 배포 관리
5. 스테이플셋 pod의 상태(순서,정보)를 유지할 수 있는 관리 방식
6. Daemonset - pod를 각 노드 당 하나씩만 배포
7. JOB 일회성 애플리케이션을 지정한 완료 횟수만큼 보장
8. cronjob 일회성 애플리케이션 반복 실행행

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # 케이스에 따라 레플리카를 수정한다.
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
        - name: php-redis
          image: gcr.io/google_samples/gb-frontend:v3
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app1
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app1
  template:
    metadata:
      labels:
        app: nginx-app1
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: app1-vol
              mountPath: /usr/share/nginx/html
      volumes:
        - name: app1-vol
          configMap:
            name: app1-html

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc1
  namespace: test
spec:
  selector:
    app: nginx-app1
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app2
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app2
  template:
    metadata:
      labels:
        app: nginx-app2
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: app2-vol
              mountPath: /usr/share/nginx/html
      volumes:
        - name: app2-vol
          configMap:
            name: app2-html

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc2
  namespace: test
spec:
  selector:
    app: nginx-app2
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP
```

```yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
        - name: pi
          image: perl:5.34.0
          command: ['perl', '-Mbignum=bpi', '-wle', 'print bpi(2000)']
      restartPolicy: OnFailure
  backoffLimit: 4
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myapp-job-para
spec:
  completions: 10
  parallelism: 3
  template:
    metadata:
      labels:
        app: myapp-job-para
    spec:
      restartPolicy: OnFailure
      containers:
        - name: sleep
          image: registry.k8s.io/busybox:1.27.2
          args:
            - /bin/sh
            - c
            - sleep 10
```

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: myapp-cj
spec:
  #  schedule: 분 시 일 월 요일
  #  completions: 10
  #  parallelism: 3
  schedule: '* * * * *'
  jobTemplate:
    spec:
      #    activeDeadlineSeconds: 10
      #   backoffLimit: 3
      template:
        metadata:
          labels:
            app: myapp-cj
        spec:
          restartPolicy: OnFailure
          containers:
            - name: sleep
              image: registry.k8s.io/busybox:1.27.2
              args:
                - /bin/sh
                - c
                - sleep 10
```

1. CLusterIP 내부용 서비스(pod 간의 통신)
   - HeadLessServic 서비스 자체에 IP 주소가 할당 X
2. NodePort 외부용 서비스(각 노드 IP 주소로 접근 (특정 포트))
3. LoadBalancer 외부용 서비스(별도의 LB 장치를 통해 접속)
   - Metal-LB 로 구현 / 클라우드는 별도의 LB 서비스 사용 / 물리 LB를 이용해 사용
4. ExternalName 1,2,3 서비스는 POD 접근 이지만 이 서비스는 pod에서 외부로 통신 시 사용 (CNAME)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
spec:
  selector:
    app: myapp-rs
  ports:
    - name: http
      port: 80 # Service Port
      targetPort: 8080 # pod Port
    - name: https
      port: 80 # Service Port
      targetPort: 8080 # pod Port
```

```yaml
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: myapp-rs-port
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - myapp-rs-port
          - backend
  #    matchLabels:
  #      tier: frontend
  template:
    metadata:
      labels:
        app: myapp-rs-port
    spec:
      containers:
        - name: myapp
          image: ghcr.io/c1t1d0s7/go-myweb:alpine
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  name: svc-port
spec:
  ports:
    - port: 80
      targetPort: http
  selector:
    app: myapp-rs-port
```

kubectl run nettool -it --image=ghcr.io/c1t1d0s7/network-multitool --rm bash

```yaml
---
apiVersion: networking.k8s.io/va
kind: Ingress
metadata:
  name: myapp-ing
spec:
  defaultBackend:
    service:
      name: myapp-svc-np
      port:
        number: 80
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-svc-np
                port:
                  number: 80
```

---

# 3일차

# Review

1. 쿠버네티스 개요
2. MSA : 모듈화 구성
3. DevOps : 개발 + 운영
4. Containers 가상화 : 프로세스 단위의 격리
   - docker, podman, containerd, cri-o (컨테이너 런타임 : 컨테이너 실행/삭제)
5. 컨테이너 오케스트레이션(관리플랫폼 : 쿠버네티스, docker swam)
6. 쿠버네티스 아키텍쳐 : controlplane, worker(node)

   - controlplane : 2 CPU, 2048(4096~) : 쿠버네티스 클러스터를 운영/관리 : 최소 1대 3대 이상(홀수:투표 시스템 때문) 권장
     - 구성요소 :
       - kube-apiserver : 명령어를 포함해 각 개체들이 통신 시 사용
       - kube-scheduler : 스케쥴링 기법 혹은 pod에 대한 리소스 할당이 있을 때 컨테이너(pod)를 실행 할 수 있는 노드를 선택해서 배치 : 정책 설정 가능
       - kube-controller-manager : 구성요소들의 관리
       - etcd :클러스터 구성 정보 등을 저장하는 저장소 (3대 이상 권장), 백업 복구도 지원은 함
       - cloud-controller-manager : 필수는 아님, 클라우드 구성요소와 연계 할 때 사용
   - node : 2CPU, 2048(운영에 필요한 만큼) : 컨테이너 실행 및 운영(리소스 제공) : 최소 2대 이상 권장(가용성을 위해) - kubelet : 노드, 컨테이터(pod) 상태 확인 하는 역할 수행 - kube-proxy : 각각의 pod에 대한 네트워크 구성 책임 - container-runtime : 컨테이너 생성, 삭제 수행

7. Workload resource
   - Pods : 애플리케이션 배포 중 최소 단위 : 1개 이상의 컨테이너로 구성 : 보편적 테스트 용으로 배포(워크로드 리소스로 배포)
   - Workload resource(controller) : replication-controller(지금은 잘 안쓰임 : deployment, replicaset으로 변환)
     - replicaset(rs) : pod의 복제본 갯수를 설정 및 유지 (절대적 갯수)
     - Deamonset(ds) : 각 노드마다 Pod를 하나씩 생성 및 유지 (절대적 갯수)
     - Job : 일회성 작업 -> 완료 횟수를 보장
     - CronJob : 일회성 작업의 주기적으로 실행행
8. label / Selector
   - Pod, Workload resource, service, node 등 개체에 대해 label 설정 가능
   - 특정 label 설정한 resource 들을 선택 하기 위해 selector 사용
   - Workload resource가 pod를 관리하기 위해
   - service를를 pod에 연결 하기 위해
   - 명령어 작업 등에서 일괄적인 선택을 위해
9. namespace(ns)
   - 각각의 리소스들을 격리하는 공간
   - application을 배포 할 때 하나의 namespce를 사용할 수 있음
10. annotaion
    - 각 개체들에 대한 주석
    - 일부 키워드(접두어)사용시 특정 기능을 제공 : 주석이 아닌 특정 기능을 사용 하기 위해 정의 되어 있음
11. probe : pod의 컨테이너에 대해 상태를 모니터링 하고 관리해 주는 것
    - 프로브 핸들러
      - HttpGet : HTTP 요청을 보내 응답오는 코드로 확인
      - TcpSocket : Session을 맺어서 확인 하는 형태
      - Exec : 이미지 내부의 실행파일 실행 후 결과 확인
    - 프로브 종류
      - livenessProbe : 이상이 있을 경우 컨테이너 재시작 : 이상이 없을 때 까지 계속 재시작
      - readinessProbe : 이상이 있을 경우 서비스와의 연결을 해제 이상이 없을 경우 서비스와 연결 : 서비스 복구시 다시 연결
      - StartupProbe : 앞에 2개의 Probe 보다 먼저 실행 되어 컨테이너의 상태를 확인 이상 유무 확인
12. Network
    - Service:
      - ClusterIP : 내부용 서비스 : Pod 끼리 통신 : 고정 IP 주소와 단일 진입점을 제공 하기 위해 사용
      - NodePort : 외부용 서비스 : 노드의 주소와 포트로 접속 : 특정 노드로 트래픽 집중이 될 수 있음 : 포트 번호 31000~32000 까지 : 포트 번호로 접근 불편함 : ClusterIP 기능이 포함
      - LoadBalancer : 외부용 서비스 : 외부 별도의 로드밸런서로 접속 : MetalLB를 통해 내부 구성도 가능 : 클라우드 : 클라우드 로드밸런서 서비스 사용 : ClusterIP나 NodePort 서비스 포함함
        ---> 파드의 진입하기 위한 서비스들
      - ExternalName : 파드에서 외부로 접근 시 사용
        - 없어도 되지만 CNAME 기능을 제공 해서 편리하게 접근 가능
      - Ingress : 파드를 외부에 노출하기 위해 사용
        - 항상 service와 pod가 존재해야 ingress가 동작
        - 노출 시 여러 그룹의 파드를 하나의 Ingress 노출 가은
        - URL / 경로 를 통해 각 그룹으로 포워딩
        - 규칙의 집합
      - IngressController : nginx 등을 이용해 구성(nginx/ingresscontroller)
      - IngressClass : IngressController와 Ingress를 연결(매핑)

# 레디니스 프로브

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-readiness
spec:
  replicas: 3
  selector:
    #    matchExpressions:
    #      - key: app
    #        operator: In
    #        values:
    #          - myapp-rs
    #          - backend
    matchLabels:
      app: myapp-rs-readiness
  template:
    metadata:
      labels:
        app: myapp-rs-readiness
    spec:
      containers:
        - name: myapp
          image: ghcr.io/c1t1d0s7/go-myweb:alpine
          readinessProbe:
            exec:
              command:
                - ls
                - /var/ready
          ports:
            - containerPort: 8080
```

```bash
ubuntu@sjh-rke-depoly:~/3day$ kubectl describe po myapp-rs-readiness-jjtdq
Name:             myapp-rs-readiness-jjtdq
Namespace:        default
Priority:         0
Service Account:  default
Node:             sjh-rke-work2/192.168.0.133
Start Time:       Wed, 12 Mar 2025 02:16:21 +0000
Labels:           app=myapp-rs-readiness
Annotations:      cni.projectcalico.org/containerID: 87588aa1ecaadbd592ca38817736cf1bd1ab1606b2ed4339d9a2bce394c05243
                  cni.projectcalico.org/podIP: 10.42.3.154/32
                  cni.projectcalico.org/podIPs: 10.42.3.154/32
Status:           Running
IP:               10.42.3.154
IPs:
  IP:           10.42.3.154
Controlled By:  ReplicaSet/myapp-rs-readiness
Containers:
  myapp:
    Container ID:   containerd://a861364672f228fc3f89b5b79dba25258708dcd4ba39e341c85f29799e61b01d
    Image:          ghcr.io/c1t1d0s7/go-myweb:alpine
    Image ID:       ghcr.io/c1t1d0s7/go-myweb@sha256:925dd88b5abbe7b9c8dbbe97c28d50178da1d357f4f649c6bc10a389fe5a6a55
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 12 Mar 2025 02:16:22 +0000
    Ready:          False
    Restart Count:  0
    Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wxdj2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-wxdj2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  3m33s                 default-scheduler  Successfully assigned default/myapp-rs-readiness-jjtdq to sjh-rke-work2
  Normal   Pulled     3m33s                 kubelet            Container image "ghcr.io/c1t1d0s7/go-myweb:alpine" already present on machine
  Normal   Created    3m33s                 kubelet            Created container: myapp
  Normal   Started    3m33s                 kubelet            Started container myapp
  Warning  Unhealthy  54s (x22 over 3m33s)  kubelet            Readiness probe failed: ls: /var/ready: No such file or directory
```

```yaml

---                                                                                                                                                                            apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-readiness
spec:
  replicas: 3
  selector:
    #    matchExpressions:
    #      - key: app
    #        operator: In
    #        values:
    #          - myapp-rs
    #          - backend
    matchLabels:
      app: myapp-rs-readiness
  template:
    metadata:
      labels:
        app: myapp-rs-readiness
    spec:
      containers:
        - name: myapp
          image: ghcr.io/c1t1d0s7/go-myweb:alpine
          readinessProbe:
            exec:
              command:
                - ls
                - /var/ready
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc-rediness
spcc:
  ports:
   - port: 80
     targetport: 8080
  selector:
    app: myapp-rs-rediness
```

# 헤드리스 서비스

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-headiness
spec:
  replicas: 3
  selector:
    #    matchExpressions:
    #      - key: app
    #        operator: In
    #        values:
    #          - myapp-rs
    #          - backend
    matchLabels:
      app: myapp-rs-headiness
  template:
    metadata:
      labels:
        app: myapp-rs-headiness
    spec:
      containers:
        - name: myapp
          image: ghcr.io/c1t1d0s7/go-myweb:alpine
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc-headless
spec:
  ClusterIP: None
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: myapp-rs-headless
```

```yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp-sts-headless
spec:
  serviceName: myapp-svc-headless
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
      app: nginx
    spec:
      containers:
        - name: nginx
          image: k8s.gcr.io/ngix-slim:0.8
```

# 스토리지 연결

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-fortune
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp-rs-fortune
  template:
    metadata:
      labels:
        app: myapp-rs-fortune
    spec:
      volumes:
        - name: web-fortune
          emptyDir: {}
      containers:
        - name: web-server
          image: nginx:alpine
          volumeMounts:
            - name: web-fortune
              mountPath: /usr/share/nginx/html
              readOnly: true
          ports:
            - containerPort: 80
        - name: html-generator
          image: ghcr.io/c1t1d0s7/fortune
          volumeMounts:
            - name: web-fortune
              mountPath: /var/htdocs
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-git
  labels:
    app: myapp-pod-git
spec:
  volumes:
    - name: git-repository
      emptyDir: {}
  initContainers:
    - name: git-clone
      image: alpine/git
      args:
        - clone
        - https://github.com/c1t1d0s7/startbootstrap-sb-admin-2.git
        - /repo
      volumeMounts:
        - name: git-repository
          mountPath: /repo
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: git-repository
          mountPath: /usr/share/nginx/html
```

```bash
sjh-rke-master1
ubuntu@sjh-rke-work1:~$ mkdir /srv/web-content/
mkdir: cannot create directory ‘/srv/web-content/’: Permission denied
ubuntu@sjh-rke-work1:~$ sudo mkdir /srv/web-content/
ubuntu@sjh-rke-work1:~$ echo "bye Hostpath Volume" | sudo tee /srv/web-content/index.html

```

```bash
sudo apt install nfs-common nfs-kernel-server -y
sudo mkdir /nfs-share
sudo chmod -R 777 /nfs-share

ubuntu@sjh-rke-depoly:~/3day$ cat /etc/exports
# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#
/nfs-share *(rw,sync,no_subtree_check,no_root_squash)
sudo exportfs -r

sudo systemctl restart nfs-server
showmount -e

ssh ubuntu@192.168.0.131 sudo apt install -y nfs-common
ssh ubuntu@192.168.0.132 sudo apt install -y nfs-common
ssh ubuntu@192.168.0.133 sudo apt install -y nfs-common

```

```bash
ubuntu@sjh-rke-depoly:~/3day$ kubectl exec myapp-rs-nfs-vlnqv -- ls /usr/share/nginx/html
ubuntu@sjh-rke-depoly:~/3day$ echo "Test nfs Volume" | sudo tee /nfs-share/index.html
Test nfs Volume
ubuntu@sjh-rke-depoly:~/3day$ kubectl exec myapp-rs-nfs-vlnqv -- ls /usr/share/nginx/html
index.html
ubuntu@sjh-rke-depoly:~/3day$ kubectl exec myapp-rs-nfs-vlnqv -- touch /usr/share/nginx/html/aaa
ubuntu@sjh-rke-depoly:~/3day$ echo "Test nfs Volume" | sudo tee /nfs-share/index.html^C
ubuntu@sjh-rke-depoly:~/3day$ kubectl exec myapp-rs-nfs-vlnqv -- ls /usr/share/nginx/html
aaa
index.html
ubuntu@sjh-rke-depoly:~/3day$ ls /nfs-share/
aaa  index.html
```

1. emptyDir : 임시 스토리지 : 파드 내부의 컨테이너 간의 데이터 공유 : 데이터는 pod 삭제 시 함께 삭제 : 어떤 노드에서 실행해도 처음에는 빈 공간 -> 동일
2. hostsPath : 영구 스토리지 중 1 : 파드의 데이터를 영구 저장 위해 사용 : 데이터는 파드를 삭제 해도 유지-> 각 노드의 로컬 스토리지를 사용 -> 각 파드가 실행하는 노드에 따라 데이터가 상이
3. nfs: 영구 스토리지 중 1 : 파드의 데이터 영구 저장 및 공유 : hostPath와 달리 어떤 노드에 쓰더라도 공유해서 사용

# pv pvc

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myapp-pv-nfs
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /nfs-share
    server: 192.168.0.130
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myapp-pvc-nfs
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
    - ReadWriteMany
  volumeName: myapp-pv-nfs
#  storageClassName: ''
```

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-nfs-pv
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-nfs-pv
  template:
    metadata:
      labels:
        app: myapp-rs-nfs-pv
    spec:
      volumes:
        - name: nfs-share
          persistentVolumeClaim:
            claimName: myapp-pvc-nfs
      containers:
        - name: web-server
          image: nginx:alpine
          volumeMounts:
            - name: web-content
              mountPath: /usr/share/nginx/html
          ports:
            - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc-nfs-pv
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: myapp-rs-nfs-pv
```

# 동적 pv pvc

[nfs-subdir-external-provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)
git clone : https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.git
cd /home/ubuntu/3day/nfs-subdir-external-provisioner/deploy

rbac.yaml : 유저, 정책 설정

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: ['']
    resources: ['nodes']
    verbs: ['get', 'list', 'watch']
  - apiGroups: ['']
    resources: ['persistentvolumes']
    verbs: ['get', 'list', 'watch', 'create', 'delete']
  - apiGroups: ['']
    resources: ['persistentvolumeclaims']
    verbs: ['get', 'list', 'watch', 'update']
  - apiGroups: ['storage.k8s.io']
    resources: ['storageclasses']
    verbs: ['get', 'list', 'watch']
  - apiGroups: ['']
    resources: ['events']
    verbs: ['create', 'update', 'patch']
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: ['']
    resources: ['endpoints']
    verbs: ['get', 'list', 'watch', 'create', 'update', 'patch']
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

```bash
buntu@sjh-rke-depoly:~/3day/nfs-subdir-external-provisioner/deploy$ kubectl create -f rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
```

스토리지 관리 인터페이스 생성성
ubuntu@sjh-rke-depoly:~/3day/nfs-subdir-external-provisioner/deploy$ cat deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.0.130 # internal my ip
            - name: NFS_PATH # internal my path
              value: /nfs-share
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.0.130 # internal my ip
            path: /nfs-share # internal my path
```

스토리지 클래스 생성
ubuntu@sjh-rke-depoly:~/3day/nfs-subdir-external-provisioner/deploy$ cat class.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: 'false'
```

ubuntu@sjh-rke-depoly:~/3day$ cat pvc.yaml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myapp-pvc-nfs
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs-client
```

---

# 4일차

1. 볼륨 : 파드는 statless 방식으로 동작 : 파드는 여러개의 컨테이너가 존재 : 파드의 데이터를 영구 저장하거나 각 컨테이너 간의 공유를 위해 사용
   - emptyDir의 경우에는 공유 기능만 제공
   - hostPath를 비롯한 나머지(일반적인 스토리지) : 공유 기능과 데이터의 영구 저장이 가능 : 각 각의 노드의 스토리지를 그대로 사용 하기 때문에 따로 스토리지 준비가 필요 하지 않음 : 노드 마다 데이터가 상의
     - NFS 를 포함한 스토리지 서비스 : PV / PVC 형태로 사용이 가능
   - 일반 볼륨 : hostPath를 포함한 PV 를 사용 하지 않는 일반 스토리지 : 볼륨 자체의 라이프 사이클이 파드와 동일(종속 됨)
     - 볼륨 -> 컨테이너 단순 연결
     - pod.spec.volumes.xxx / pod.spec/containers.volumeMounts
   - PV : 볼륨(PV) 자체의 라이프 사이클이 파드와 별개로 관리
     - PVC를 통해 파드와 연결
     - 수동 스토리지 설정(StorageClass의 이름이 불 필요) : 실제 스토리지 - PV - PVC - Pods(컨테이너)
     - PV, PVC 를 만들어야 함
     - pod.spec.volumes.pvc / pod.spec/containers.volumeMounts
     - 동적 스토리지 설정(StorageClass의 이름이 필요) : 실제 스토리지 - PV - SC(연결 고리 역할) - PVC - Pods(컨테이너) :

# 컨피그맵

# secret

```bash
P@ssw0rdubuntu@sjh-rke-depoly:~/4day$ echo -n "admin" > id.txt
ubuntu@sjh-rke-depoly:~/4day$ echo test > fileA
ubuntu@sjh-rke-depoly:~/4day$ echo -n "P@assw0rd" > pwd.txt
ubuntu@sjh-rke-depoly:~/4day$ kubectl create secret generic my-secret --from-file id.txt --from-file pwd.txt
secret/my-secret created
ubuntu@sjh-rke-depoly:~/4day$ kubectl get secrets
NAME        TYPE     DATA   AGE
my-secret   Opaque   2      8s
ubuntu@sjh-rke-depoly:~/4day$ kubectl get secrets my-secret -o yaml
apiVersion: v1
data:
  id.txt: YWRtaW4=
  pwd.txt: UEBhc3N3MHJk
kind: Secret
metadata:
  creationTimestamp: "2025-03-13T05:29:13Z"
  name: my-secret
  namespace: default
  resourceVersion: "1362786"
  uid: ad7e84b4-5c01-42a1-891c-cd448ae647ae
type: Opaque
ubuntu@sjh-rke-depoly:~/4day$ kubectl describe secrets my-secret
Name:         my-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
id.txt:   5 bytes
pwd.txt:  9 bytes
```

# 디플로이먼트

- 레플리카셋의 상위 개체
- 레플리카셋으로 생성하는 파드의 버전 및 배포 정책관리
- 버전마다 별도의 레플리카 셋으로 관리
- 그 중에 하나의 레플리카셋 및 파드만 활성화
- 각 버전마다 기록을 남기게 되는데
  - 명령어 옵션으로 --record 사용
  - 디폴로이먼트 부분의 metadata: annotation 설정으로 메시지 지정 기능
    - kubernetes.io/change-cause:xxxxx 식으로 작성
    - yaml replace, apply 명령어를 통해서만 기록 가능
  - 각 버전의 리비전 번호로 저장 됨
  - 각 버전의 리비전 번호로 rollback이 가능
  - 각 버전의 리비전 번호는 수정 하거나 rollback했을 때 언제든지 번호가 증가 함

# 스테이트풀셋

# 5일차

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: myapp-limitrange
spec:
  limits:
    - type: Pod
      min:
        cpu: 50m
        memory: 5Mi
      max:
        cpu: 1
        memory: 1Gi
    - type: Container
      defaultRequest:
        cpu: 100m
        memory: 10Mi
      default:
        cpu: 200m
        memory: 100Mi
      min:
        cpu: 50m
        memory: 5Mi
      max:
        cpu: 1
        memory: 1Gi
      maxLimitRequestRatio:
        cpu: 4
        memory: 10
    - type: PersistentVolumeClaim
      min:
        storage: 10Mi
      max:
        storage: 1Gi
```

```bash
Name:                  myapp-limitrange
Namespace:             default
Type                   Resource  Min   Max  Default Request  Default Limit  Max Limit/Request Ratio
----                   --------  ---   ---  ---------------  -------------  -----------------------
Pod                    memory    5Mi   1Gi  -                -              -
Pod                    cpu       50m   1    -                -              -
Container              cpu       50m   1    100m             200m           4
Container              memory    5Mi   1Gi  10Mi             100Mi          10
PersistentVolumeClaim  storage   10Mi  1Gi  -                -              -
```

# 노드

```bash
 --- 노드 기준
1. 노드 네임 : 특정 노드의 이름으로 파드를 배치할 곳을 지정
2. 노드 셀렉터 : 노드의 설정한 레이블을 선택해서 배치 (강제적 느낌)
3. 노드 어피니티 : 노드의 레이블을 선택해 배치 (권장 느낌)
  --- 파드 기준
4. 파드 어피니티 : 기존/실행하려는 파드들이 같은 곳에 배치
   - 사전에 실행해둔 파드의 레이블 확인 후 해당 파드와 동일한 노드에 배치
   - 사전에 실행해둔 파드가 없으면 자유롭게 배치
   - 현재 실행하려는 파드의 레이블을 지정하면 모두 동이란 노드에 배치
5. 파드 안티 어피티니 : 기존/실행 하려는 파드들을 분산 배치
   - 사전에 실행해둔 같은 레이블의 파드가 있을 경우 해당 파드를 피해서 배치
   - 사전에 실행해둔 파드가 없으면 자유롭게 배치
   - 현재 실행하려는 파드의 레이블을 지정하면 서로 다른 노드에 배치
  --- 옵션들
6. 테인트 : 노드의 레이블 : 노드를 배치하지 않도록 설정 : 특정 키에 맞게 배치
7. 톨러레이션 : 테인트 설정에 대한 예외 처리
8. 커튼 : 스케줄링 차단
9. 드레인 : 커든 + 지금 실행 중인 Pod 까지 종료 -> 갯수가 정해져 있다면 다른 노드에 배치 됨
```

# 커든

```bash
ubuntu@sjh-rke-depoly:~/5day$ kubectl describe node |grep -A1 Taint
Taints:             test=test1:NoSchedule
Unschedulable:      false
--
Taints:             test=test1:NoSchedule
Unschedulable:      false
--
Taints:             node.kubernetes.io/unschedulable:NoSchedule
                    test=test1:NoSchedule
--
Taints:             test=test1:NoSchedule
Unschedulable:      false
ubuntu@sjh-rke-depoly:~/5day$
ubuntu@sjh-rke-depoly:~/5day$
ubuntu@sjh-rke-depoly:~/5day$ kubectl uncordon sjh-rke-work1
node/sjh-rke-work1 uncordoned
ubuntu@sjh-rke-depoly:~/5day$ kubectl describe node |grep -A1 Taint
Taints:             test=test1:NoSchedule
Unschedulable:      false
--
Taints:             test=test1:NoSchedule
Unschedulable:      false
--
Taints:             test=test1:NoSchedule
Unschedulable:      false
--
Taints:             test=test1:NoSchedule
Unschedulable:      false
```

# 드레인

```bash
kubectl drain sjh-rke-work1
node/sjh-rke-work1 cordoned
error: unable to drain node "sjh-rke-work1" due to error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/rke2-canal-8kw4q, kube-system/rke2-ingress-nginx-controller-5r8m8, metallb-system/metallb-speaker-rp9dg, continuing command...
There are pending nodes to be drained:
 sjh-rke-work1
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/rke2-canal-8kw4q, kube-system/rke2-ingress-nginx-controller-5r8m8, metallb-system/metallb-speaker-rp9dg

ubuntu@sjh-rke-depoly:~/5day$ kubectl drain sjh-rke-work1 --ignore-daemonsets
node/sjh-rke-work1 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/rke2-canal-8kw4q, kube-system/rke2-ingress-nginx-controller-5r8m8, metallb-system/metallb-speaker-rp9dg
evicting pod kube-system/rke2-coredns-rke2-coredns-55bdf87668-gpbxv
evicting pod default/myapp-rs-notol-q9p4p
evicting pod default/myapp-rs-notol-xcbnq
evicting pod default/myapp-rs-notol-6kmwz
pod/myapp-rs-notol-6kmwz evicted
pod/myapp-rs-notol-q9p4p evicted
pod/myapp-rs-notol-xcbnq evicted
^[[Apod/rke2-coredns-rke2-coredns-55bdf87668-gpbxv evicted
node/sjh-rke-work1 drained


ubuntu@sjh-rke-depoly:~/5day$ kubectl describe node sjh-rke-work1
Name:               sjh-rke-work1
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=rke2
                    beta.kubernetes.io/os=linux
                    gpu=true
                    gpu-model=h100
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=sjh-rke-work1
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=rke2
Annotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.132
                    flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"72:08:fd:75:b2:86"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.0.132
                    management.cattle.io/pod-limits: {}
                    management.cattle.io/pod-requests: {"cpu":"600m","memory":"218Mi","pods":"4"}
                    node.alpha.kubernetes.io/ttl: 0
                    rke2.io/hostname: sjh-rke-work1
                    rke2.io/internal-ip: 192.168.0.132
                    rke2.io/node-args: ["agent","--server","https://192.168.0.130:9345","--token","********"]
                    rke2.io/node-config-hash: 3JP5FAV2EXEKIJKHCVM47YTZGO7G5BWD5TILGV2XXJUD7HSWQWXA====
                    rke2.io/node-env: {}
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Mar 2025 02:57:39 +0000
Taints:             node.kubernetes.io/unschedulable:NoSchedule
                    test=test1:NoSchedule
Unschedulable:      true
Lease:
  HolderIdentity:  sjh-rke-work1
  AcquireTime:     <unset>
  RenewTime:       Fri, 14 Mar 2025 06:46:04 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Mon, 10 Mar 2025 02:59:55 +0000   Mon, 10 Mar 2025 02:59:55 +0000   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Fri, 14 Mar 2025 06:46:03 +0000   Mon, 10 Mar 2025 02:57:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 14 Mar 2025 06:46:03 +0000   Mon, 10 Mar 2025 02:57:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 14 Mar 2025 06:46:03 +0000   Mon, 10 Mar 2025 02:57:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 14 Mar 2025 06:46:03 +0000   Mon, 10 Mar 2025 02:59:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.0.132
  Hostname:    sjh-rke-work1
Capacity:
  cpu:                16
  ephemeral-storage:  103668328Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16331804Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  100848549400
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16331804Ki
  pods:               110
System Info:
  Machine ID:                 190fa7311bf44d64b717bd95b928a4d8
  System UUID:                190fa731-1bf4-4d64-b717-bd95b928a4d8
  Boot ID:                    3b08acf9-0975-4e83-b796-270cd970fda2
  Kernel Version:             5.15.0-122-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://2.0.2-k3s2
  Kubelet Version:            v1.31.6+rke2r1
  Kube-Proxy Version:         v1.31.6+rke2r1
PodCIDR:                      10.42.1.0/24
PodCIDRs:                     10.42.1.0/24
ProviderID:                   rke2://sjh-rke-work1
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  kube-system                 kube-proxy-sjh-rke-work1               250m (1%)     0 (0%)      128Mi (0%)       0 (0%)         4d3h
  kube-system                 rke2-canal-8kw4q                       250m (1%)     0 (0%)      0 (0%)           0 (0%)         4d3h
  kube-system                 rke2-ingress-nginx-controller-5r8m8    100m (0%)     0 (0%)      90Mi (0%)        0 (0%)         26m
  metallb-system              metallb-speaker-rp9dg                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                600m (3%)   0 (0%)
  memory             218Mi (1%)  0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason              Age                    From     Message
  ----    ------              ----                   ----     -------
  Normal  NodeSchedulable     5m27s (x2 over 6m58s)  kubelet  Node sjh-rke-work1 status is now: NodeSchedulable
  Normal  NodeNotSchedulable  4m5s (x3 over 7m8s)    kubelet  Node sjh-rke-work1 status is now: NodeNotSchedulable

```

---
